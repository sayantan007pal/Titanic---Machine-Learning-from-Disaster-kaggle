{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   PassengerId  Survived  Pclass  \\\n",
       " 0            1         0       3   \n",
       " 1            2         1       1   \n",
       " 2            3         1       3   \n",
       " 3            4         1       1   \n",
       " 4            5         0       3   \n",
       " \n",
       "                                                 Name     Sex   Age  SibSp  \\\n",
       " 0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       " 1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       " 2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       " 3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       " 4                           Allen, Mr. William Henry    male  35.0      0   \n",
       " \n",
       "    Parch            Ticket     Fare Cabin Embarked  \n",
       " 0      0         A/5 21171   7.2500   NaN        S  \n",
       " 1      0          PC 17599  71.2833   C85        C  \n",
       " 2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       " 3      0            113803  53.1000  C123        S  \n",
       " 4      0            373450   8.0500   NaN        S  ,\n",
       "    PassengerId  Pclass                                          Name     Sex  \\\n",
       " 0          892       3                              Kelly, Mr. James    male   \n",
       " 1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       " 2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       " 3          895       3                              Wirz, Mr. Albert    male   \n",
       " 4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       " \n",
       "     Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       " 0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       " 1  47.0      1      0   363272   7.0000   NaN        S  \n",
       " 2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       " 3  27.0      0      0   315154   8.6625   NaN        S  \n",
       " 4  22.0      1      1  3101298  12.2875   NaN        S  ,\n",
       "    PassengerId  Survived\n",
       " 0          892         0\n",
       " 1          893         1\n",
       " 2          894         0\n",
       " 3          895         0\n",
       " 4          896         1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_file_path = 'train.csv'\n",
    "test_file_path = 'test.csv'\n",
    "gender_submission_file_path = 'gender_submission.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "gender_submission_df = pd.read_csv(gender_submission_file_path)\n",
    "\n",
    "# Display the first few rows of the datasets to understand their structure\n",
    "train_df.head(), test_df.head(), gender_submission_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8044692737430168, 'titanic_predictions_clean_logistic.csv')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define preprocessing steps\n",
    "numeric_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine preprocessors in a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create the model pipeline with preprocessing and Logistic Regression\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "model_pipeline.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Evaluate the model\n",
    "y_val_pred_clean = model_pipeline.predict(X_val)\n",
    "val_accuracy_clean = accuracy_score(y_val, y_val_pred_clean)\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions_clean = model_pipeline.predict(X_test)\n",
    "\n",
    "# Prepare the submission file\n",
    "submission_df_clean = test_df[['PassengerId']].copy()\n",
    "submission_df_clean['Survived'] = test_predictions_clean\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output_file_path_clean = 'titanic_predictions_clean_logistic.csv'\n",
    "submission_df_clean.to_csv(output_file_path_clean, index=False)\n",
    "\n",
    "val_accuracy_clean, output_file_path_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define preprocessing steps\n",
    "numeric_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessors in a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the model pipeline with preprocessing and Logistic Regression\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the pipeline on the training data\n",
    "model_pipeline.fit(X_train_split, y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "y_val_pred_clean = model_pipeline.predict(X_val)\n",
    "val_accuracy_clean = accuracy_score(y_val, y_val_pred_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "test_predictions_clean = model_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the submission file\n",
    "submission_df_clean = test_df[['PassengerId']].copy()\n",
    "submission_df_clean['Survived'] = test_predictions_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions to a CSV file\n",
    "output_file_path_clean = 'titanic_predictions_clean_logistic.csv'\n",
    "submission_df_clean.to_csv(output_file_path_clean, index=False)\n",
    "\n",
    "val_accuracy_clean, output_file_path_clean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
